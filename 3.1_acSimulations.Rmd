# Setup
```{r setup, include = FALSE}
  rm(list = ls())

knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, cache = TRUE, fig.align = 'center', fig.width = 10, fig.height = 7) 

# libraries
library(tidyverse)
library(readxl)
library(magrittr)
library(MASS)
library(matrixStats)
library(dplyr)
library(data.table)
library(gtable)
library(gridExtra)
library(sp)
library(raster)
library(rgeos)
library(rgdal)
library(sf)

# data
# BRtrips <- readRDS("./data/gpkg/BRtrips_KR_4283.gpkg") # Korinna Ryans BR estimates
# BRtrips <- readRDS("./data/gpkg/2.1_BRtrips.gpkg") # This is produced by "BRtrips.R" in function folder

# simulation 
model <- "nex" #'[#NOTE: choose model] 
sim <- "sim4" #'[#NOTE: choose simulation] 
cost <- "fcflt_spgam"  #'[#NOTE: choose cost variable] # not needed for attatasc

b <- read_excel(paste0("./data/03_data/", model, ".xlsx")) # coefs for model (marginal utilities)
a <- nrow(b) + 2
v <- read_excel(paste0("./data/03_data/", model, ".xlsx"), sheet = "v", col_names = rep("x", a)) %>% distinct() # covariance matrix for drawing errors from a mutivariate random normal distribution

# dat <- readRDS(paste0("./data/gpkg/sims/2.1_ascCS_rum_", sim ,"_4283.gpkg")) # for when no data is models eg. catch rate
# dat <- read_csv(paste0("./data/02_data/2.3_asc_dat_", sim, ".csv")) # for when  data is models eg. catch rate
# dat <- read_csv("./data/02_data/2.3_dat_att.csv") # attribute

# NEX RUMN paper
dat <- read_csv(paste0("./data/02_data/2.1_nex_", sim, ".csv")) # NEX
# dat <- read_csv(paste0("./data/02_data/2.1_nex_ex_", sim, ".csv")) # comprable EX for NEX

# plotting data
# mp <- st_read("./data/gpkg/pmmp_4326.gpkg")
# # mp_bbox <- st_read("./data/gpkg/pscmpBbox_4326.gpkg")
# NPZ <- st_read("./data/gpkg/ntz_4283.gpkg")

# source
# source("./functions/theme.R")
```

# Wrangle data
dat: contains a row for  (nrow(dat)) = every available option (n grid cells, excluding current sz and sim sz with no recreational value) * per trip (n trips). The choice column indicated what site the recreator actually chose. 


```{r rearrange, include = FALSE}
    #'[#VALIDATE: TRUE, check dat has correct number of rows]
    # nrow(dat) == n_distinct(dat$gridID_alt) * n_distinct(dat$TripID) not as simple for nexso doesnt match

    # grid <- dat %>% as.data.frame() %>%
    #     distinct(gridID_alt, .keep_all = TRUE) %>% # nrow(grid) == number of available options
    #     dplyr::select(gridID_alt, asc_geom) %>%
    #     st_as_sf()
    
    b %<>% rename(cov = ...1) %>% 
      # mutate(gridid_alt = ifelse(Vars %in% "travelcost", "tc", gridid_alt)) %>% 
      mutate(cov = ifelse(!(Vars %in% "_cons"), Vars, cov)) %>% 
      dplyr::select(-Vars)
  
    v %<>% 
      # mutate(x...1 = ifelse(x...2 %in% "travelcost", "tc", x...1)) %>%  
      mutate(x...1 = ifelse(!(x...2 %in% "_cons"), x...2, x...1)) %>%  
      dplyr::select(-x...2) %>% 
      distinct()
  
    colnames(v) <- c("vars", v$x...1)
    v %<>% dplyr::select(-vars)
    vars <- b$cov #use

    #'[#VALIDATE: TRUE/TRUE/check min and max, extreme values will skew model (issue 2.1?)]
    nrow(b) == length(v) # should be true
    length(v) == nrow(v) # should be true
    summary(b$Coef) # check min and max, the more extreme the values the more inflated or deflated the results will be. Most likely a problem with script 2.1 or model.
```

# Error 
Generates coefficients drawn from within the ci of each variable. Draws n1 numbers (nrow) from within a multivariate normal distribution for each variable (length); mu = vector of means, Sigma = covarience matrix
```{r error, include = FALSE}
    n1 <- 1000 # number of samples (all estimates)
    mvn_b <- mvrnorm(n1, mu = b$Coef, Sigma = as.matrix(v)) 

# checkpoint
dim(mvn_b)[1] == n1 # should be true
dim(mvn_b)[2] == nrow(b) # should be true
```

# Utility matrix

The coef of the logit model gives us the marginal utilities. This is the extra utility an agent receives for one extra unit, given all the other parameters stay constant. 

Calculate the utility (Vj) for every alternative, each individual observation in dat represents an alternative. 

We expect recreator  (n) to choose the site (j) with the highest expected utility. The utility  (Vj) of a site is a function of the observed attributes (B) of those sites and associated error (e).

      Vnj = Bj + Ej

      Vj = B(depth + travel cost etc.) + e 
      
The errors are assumed to follow the generalized extreme value (GEV) distribution type 1 (Gumbel distribution). This distribution is a normal distribution with a slight larger tail on one side. The errors are also assumed to be identical and independently distributed (i.i.d. assumption). This means there cannot be any correlation in the errors. Therefore the site definitions have to have no correlated errors, and therefore have to be different enough that the error don't correlated. If the researcher thinks there is some unobserved correlation in the errors, then the alternatives can be grouped into nests. For example boat fishing and shore fishing and inherently different, therefore you could cluster shore and boat based sites into a boat and shore nest. Doing a nested model relaxes the iid assumption and allows for some correlation within nests, but no correlation between nests. 

```{r utility, include = FALSE}
# calculate utility
# add f_ from binary vars 

# dat %<>% rename(f_pred_catch_bin_fit = pred_catch_bin_fit)

      # define vars used in RUM
      # bid <- unique(b$cov) 

# FISHING RUM when there are attributes
      # id_vars <- dat %>% dplyr::select(num_range("gridid_", bid)) # extract grid IDs
      
      # bsite <- bid[str_detect(bid, "[a-zA-Z]")] 
      # # bsite <- c("fcflt_spgam", "p_dem.fit", "swell", "arealog") # temp manual to add factor
      # site_vars <- dat %>% dplyr::select(contains(bsite)) # extract attributes
      # 
      # vars <- c(names(site_vars), names(id_vars)) #'[#NOTE: add factor vars]
      
 # turn defined vars into matrix from dat, make sure there is no geometry
      # rum_matrix <- as.matrix(dat[, vars])

# ASC NEX RUM
      b %<>% mutate(cov = ifelse(!str_detect(cov, "fc"), paste0("gridid_", cov), cov)) # NEX 
      bid <- unique(b$cov) 
      rum_matrix <- as.matrix(dat[, bid])
    
    # create an empty utility matrix
        Vj <- matrix(NA, nrow = nrow(rum_matrix), ncol = n1)
        
# calculate observed utility for every alternative (nrows), n1 times from multivariate random normal distribution. We are multiplying the values in the rum_matrix (real data) by the error coefficients matrix (mvn_b)? so we have n1 utility values for each variable. 
    
# The loop below often fails: non-confromable. This means that you need to check the names in the vectors above to make sure the are the same as the model and mach the data set in the right order
        
        for (i in 1:n1) { 
          Vj[ ,i] <- as.vector(rum_matrix %*% as.matrix(mvn_b[i,]))  
        }
        
    dat %<>%
        mutate(gridid_int = group_indices(., gridid_alt),
         tripid_int = group_indices(., trip_id))  # making id integers    
        
        
    Vj <- as.data.table(Vj) # make data.table to save RAM
    cols <- colnames(Vj) # save the names of the draws for later
    Vj$tau <- dat$tau
    Vj$ID <- as.integer(dat$gridid_int) # run function by BR
    Vj$tc <- dat$fcflt_spgam # add travel cost
    Vj$trip_id <- as.integer(dat$trip_id) # add trip id
    Vj$br <- as.integer(dat$ramp_id) # add trip id
    
    nx <- length(Vj) - n1
    nx <- length(Vj) - nx
        
    condp_Vj <- Vj %>%  mutate(across(1:nx, ~ . / tau)) # divide utilities by tau - NESTED LOGIT ONLY
    # Vj %<>%  mutate(across(1:nx, ~ . / tau)) # divide utilities by tau - NESTED LOGIT ONLY
  
    ## FOR ALL
     condp_Vj %<>% mutate(across(1:nx, exp)) # get exponential of utility
     # Vj %<>% mutate(across(1:nx, exp)) # get exponential of utility
    
        # Vj <- exp(Vj) # get exponential of utility
        
        #'[#VALIDATE: TRUE, exp(Vj) should make all values positive]
       identical(which(Vj < 0), integer(0))
       which(is.infinite(as.matrix(Vj)))
     
     identical(which(condp_Vj < 0), integer(0))
      which(is.infinite(as.matrix(condp_Vj)))
    
       # data for testing manually   
       # t <- dat %>% filter(trip_id %in% 259)
       # t <- t[, c("trip_id", "choice", "fcflt_spgam", "cset", "gridid_alt", "tau")]
       # t %<>% arrange(gridid_alt)
       # write.csv(t, "./data/03_data/test.csv")
```

# Choice probability

The probability of a recreator (i) choosing site (j) is the exponential of the the utility of the site (j) divided by the sum of the utility of all the potential alternatives the recreator could have choosen (including site j). This is a conditional logit formula. 

    probj = exp(Vj)/sum(exp(Vj))
    
Conditional choice probability
We need to use the condition choice probability when doing nested models. This is the probability that the agent choose the bottom level alternative given they choose the nest. 

## Pr status quo
```{r Pr_base, include = FALSE}
    # Calculate probability of visit and logsums: pre-simulation
    f1 <- function(x) if (is.integer(x)) {x} else {x/sum(x)}

    p <- Vj[ , lapply(.SD, f1), by = br, .SDcols = c(cols, "ID")] # applying probability function to every cell in Vj by boat ramp - BOAT BASED RUMS ONLY
    condp <- condp_Vj[ , lapply(.SD, f1), .SDcols = c(cols)] # applying probability function to every cell in Vj - no groupd when BOAT AND SHORE BASED should be same length as data

    logsum_condp <- condp_Vj[ , lapply(.SD, sum), by = trip_id, .SDcols = cols]
    
    logsum_condp <- log(logsum_condp[, -1])
    w <- as.matrix(logsum_condp)
    logsum_condp$mean_logsum <- rowQuantiles(w, probs = sqrt(0.025)) # lower ci
    
    
    # p <- p[order(br, ID)] # ordering
    # dat %<>% arrange(ramp_id, gridid_alt) # ordering

    #'[#VALIDATE: TRUE, should be the same so you can append probabilities]
    # table(dat$gridid_alt == p$ID)
    # table(dat$gridid_int == p$ID)

    nrow(dat) == nrow(p)

    # p <- p[ , br := NULL] # remove
    p <- p[ , ID := NULL] # remove
    p <- as.matrix(p)   # make matrix

    #'[#VALIDATE: TRUE, sum(p(each site from br)) == 1, so sum(p(each site from every         boat ramp)) == #boat ramps]

    sum(p[,1]) # the sum of probability shoudl equal 1
    # n_distinct(dat$ramp_id)

    # Adds probability statistics (lower ci, median, upper ci) to each row of dat. This is the probability that people will visit site (gridID_alt) pre-simulation.
    # Summary stats are calculated across each row in p (rowQuantiles), length == n1

    dat$p.mean_base <- rowQuantiles(p, probs = sqrt(0.5)) # lower ci
    dat$p.low_base <- rowQuantiles(p, probs = sqrt(0.025)) # lower ci
    dat$p.upp_base <- rowQuantiles(p, probs = 1 - sqrt(0.025)) # upper ci

## Prep logsums
## Sum by boat ramp to scale the trip costs to annual boat ramp visists
    # logsum_base <- Vj[ , lapply(.SD, sum), by = br, .SDcols = cols]  # BOAT BASED

## MIXED MODE
## Sum boat and shore based (mixed mode) models by trip id to get a welafre value per person which can be averaged
    logsum_base <- Vj[ , lapply(.SD, sum), by = trip_id, .SDcols = cols]
```

## Pr counterfactual
```{r Pr_sim, include = FALSE}
# Assign closures
  # sz_vars <- dat %>% 
  #   dplyr::select(gridid_alt, sz) %>% 
  #   filter(sz == 1) %>% 
  #   distinct(gridid_alt)

# ntz <- unique(sz_vars$gridid_alt) 

## NEX  and NEX EX
 sz_vars <- dat %>%
    dplyr::select(gridid_int, zone_rm) %>%
    filter(zone_rm == 1)  %>%
    distinct(gridid_int)

   ntz <- unique(sz_vars$gridid_int)

# Calculate probability of visit and logsums: post-simulation
    Vj[Vj$ID %in% ntz, cols] <- 0
    # p <- Vj[ , lapply(.SD, f1), by = br, .SDcols = c(cols, "ID")] # BOAT BASED
    p <- Vj[ , lapply(.SD, f1), .SDcols = c(cols, "ID")] # BOAT AND SHORE
    
    # p <- p[order(br, ID)]
    # dat %<>% arrange(ramp_id, gridid_alt)
    # table(dat$gridid_alt == p$ID) # should all be tru
    # table(dat$gridid_int == p$ID) # should all be tru
    # 
    # p <- p[ , br := NULL]
    p <- p[ , ID := NULL]
    p <- as.matrix(p)
    
# Calculates probability of visit summary statistics by row. This is the simulated probability of each site, for each trip and the error. 

    dat$p.mean_sim <- rowQuantiles(p, probs = sqrt(0.5)) # lower ci
    dat$p.low_sim <- rowQuantiles(p, probs = sqrt(0.025)) #saving outputs back to p
    dat$p.upp_sim <- rowQuantiles(p, probs = 1 - sqrt(0.025)) #Note below about weird probs
    
    # Sum the utilities by grouping variable
## BOAT BASED
## Sum by boat ramp to scale the trip costs to annual boat ramp visists
    # logsum_sim <- Vj[ , lapply(.SD, sum), by = br, .SDcols = cols]  # BOAT BASED

## MIXED MODE
## Sum boat and shore based (mixed mode) models by trip id to get a welafre value per person which can be averaged
    logsum_sim <- Vj[ , lapply(.SD, sum), by = trip_id, .SDcols = cols]
```

# Extract Welfare
To estimate the change in welfare you need to log the denominator (sum(exp(utility of all alts)) of the probability function and divide by the travel cost.

welfare = log(sum(exp(utility of all alts)))/tc

welfare with a nested logit = log(sum(exp(utility of all alts/tau)))/tc

```{r welfare, include = FALSE}
# Extracting the welfare impact
    W <- log(logsum_sim[, -1]) -  log(logsum_base[, -1]) # always subtract the status quo by the counterfactual (simulation)
    W <- t(W)/mvn_b[ ,1] # divide the logsums by the travel cost (need to pivot to do this t())
    W <- t(W) # pivot back to original orientation
    
# Summaries across draws
    W <- as.matrix(W)
    logsum_sim$w.mean <- rowQuantiles(W, probs = sqrt(0.5)) # median
    logsum_sim$w.low <- rowQuantiles(W, probs = sqrt(0.025)) # lower ci
    logsum_sim$w.upp <- rowQuantiles(W, probs = 1 - sqrt(0.025)) # upper ci
    
    # dat %<>%  mutate(rampid = as.integer(rampid))
    # dat %<>% left_join(logsum_sim[ , c("br", "w.mean", "w.upp", "w.low")], by = c("ramp_id" = "br")) # BOAT
    
    dat %<>% left_join(logsum_sim[ , c("trip_id", "w.mean", "w.upp", "w.low")]) # MIXED MODE

    # # Welfare impact of closure 
    # BOAT BASED - you sum to get the total welafre impact acorss all boat ramps
    # W_impact_trip <- logsum_sim %>%
    #   summarise("w.low.trip ($)" = round(sum(w.low), 2),
    #             "w.mean.trip ($)" = round(sum(w.mean), 2),
    #             "w.ipp.trip ($)" = round(sum(w.upp), 2))

    # MIXED MODE - you average to get the average trip impact across all individuals
    W_impact_trip <- logsum_sim %>%
      summarise("w.low.trip ($)" = round(mean(w.low), 2),
                "w.mean.trip ($)" = round(mean(w.mean), 2),
                "w.upp.trip ($)" = round(mean(w.upp), 2))

    W_impact_trip
    # png("./plots/ASC RUM/ac_sim_og_impact_trip.png", height = 50*nrow(W_impact_trip), width =
    #       200*ncol(W_impact_trip))
    # grid.table(W_impact_trip)
    # dev.off()

# save output
  # write.csv(dat, "./ignore/03_data/3.1_Wsimdat.csv")
  write.csv(W_impact_trip, paste0("./data/03_data/3.1_trip_impact_", sim, "_", model, ".csv"))
```
## Annual Impact

This chunk merges estimates of trip numbers at each ramp each annually (BRtrips) with estimates of the probability of visiting each grid (by annually) given a particular boat ramp was selected (dat). 

-   Number of annual trips per br \* the W impact of each br

Multiplied br numbers by corresponding probabilities. Note that because we adjusted the probs above to sqrt(0.025) we should now have accurate 95% CIs. 

```{r br multiplication gravity, include = FALSE}

dat %<>% left_join(.,BRtrips, by = c("rampid" = "RampID"), suffix = c("", "_ramp"))

    RUM_W <- dat %>% distinct(.,rampid, .keep_all = TRUE) %>%
      # gravity function
      mutate(across(starts_with("TRIPS_ramp_prop"), ~.x*w.mean, .names = "welf_mean_{.col}" ),
             across(starts_with("upp_TRIPS"), ~.x*w.upp, .names = "welf_upp_{.col}" ),
             across(starts_with("low_TRIPS"), ~.x*w.low, .names = "welf_low_{.col}" ))
      # dprid
      # mutate(across("dprid_br_mean", ~.x*w.mean, .names = "welf_mean_{.col}" ),
      #        across("dprid_br_uci", ~.x*w.upp, .names = "welf_upp_{.col}" ),
      #        across("dprid_br_lci", ~.x*w.low, .names = "welf_low_{.col}" ))

# Annual Impact
    grav_impact <- RUM_W %>% summarise(round(across(starts_with(c("welf_low_low", "welf_mean_TRIPS","welf_upp_upp")), sum)))
 
    # dprid_impact <- RUM_W %>% summarise(round(across(starts_with(c("welf_low_dprid", "welf_mean_dprid","welf_upp_dprid")), sum)))
    
    # w_impact <- as.data.frame(cbind(grav_impact, dprid_impact)) %>% 
    #   pivot_longer(cols = 1:6) %>% 
    #   rename(value = name,
    #          ann_impact = value)
  
   grav_impact
    
    # write_csv(w_impact, paste0("./data/03_data/3.1_ann_impact_", sim,".csv"))
```

# Welfare impacts
```{r totW, include = FALSE}
W_impact <- RUM_W %>% summarise(round(across(starts_with(c("welf_low", "welf_mean","welf_upp")), sum)))

W_impact %<>%  rename("Annual lower impact ($)" = welf_low_low_TRIPS_Grvt__5,
                      "Annual mean  impact ($)" = welf_mean_TRIPS_ramp_prop_Grvt__5,
                      "Annual upper impact ($)" = welf_upp_upp_TRIPS_Grvt__5)

W_impact
png("./plots/ASC RUM/ac_s1.W_impact.png", height = 50*nrow(W_impact), width = 200*ncol(W_impact))
grid.table(W_impact)
dev.off()

```

```{r sc_spimpact, include = FALSE}
RUM_grid %<>%
     mutate(mean_diff = mean_sim_TRIPS_ramp_prop_Grvt__5 - mean_base_TRIPS_ramp_prop_Grvt__5,
            upp_diff = upp_sim_upp_TRIPS_Grvt__5 - upp_base_upp_TRIPS_Grvt__5,
            low_diff = low_sim_low_TRIPS_Grvt__5 - low_base_low_TRIPS_Grvt__5)
```


```{r}
presim <- ggplot(RUM_grid) +
      xlab("Longitude") + ylab("Latitude") +
      geom_sf(aes(fill = mean_base_TRIPS_ramp_prop_Grvt__5), lwd = 0.05) +
      scale_fill_distiller(palette = "YlGnBu", direction = 1) +
      labs(fill = "Trips\nannually") +
  my_theme +
   # geom_sf(data = BRtrips, color = 'red', fill = 'red', size = 0.75) +
  ggtitle(" Pre-simulation ")

RUM_grid %<>% filter(mean_diff >= 0)

postsim <- ggplot(RUM_grid) +
      xlab("Longitude") + ylab("Latitude") +
      geom_sf(aes(fill = mean_sim_TRIPS_ramp_prop_Grvt__5), lwd = 0.05) +
      scale_fill_distiller(palette = "YlGnBu", direction = 1) +
      labs(fill = "Trips\nannually") +
  my_theme +
   ggtitle(" Post-simulation ")
  # geom_sf(data = mp, alpha = 0, lwd = 0.25) +
  # geom_sf(data = NPZ, lwd = 0.25) +
  # geom_sf(data = sp_mpbr, color = 'red', fill = 'red', size = 0.5)

diff <- ggplot(RUM_grid) +
      xlab("Longitude") + ylab("Latitude") +
      geom_sf(aes(fill = mean_diff), lwd = 0.05) +
      scale_fill_distiller(palette = "YlGnBu", direction = -1) +
      labs(fill = "Change in\nannual trips") +
      my_theme 
    # geom_sf(data = mp, alpha = 0, lwd = 0.25) +
    # geom_sf(data = NPZ, lwd = 0.25) +
  # geom_sf(data = BRtrips, color = 'red', fill = 'red', size = 0.5) 
  # geom_sf(data = sim, lwd = 0.05, fill = "black")

presim
postsim
diff

```

# save outputs
```{r}
ggsave(presim, path = "./plots/ASC RUM", filename = paste("acPresim_", sim, ".png", sep = ''), width = 4, height = 4)

ggsave(postsim, path = "./plots/ASC RUM", filename = paste("acPostsim_", sim, ".png", sep = ''), width = 4, height = 4)

ggsave(diff, path = "./plots/ASC RUM", filename = paste("acDiff_", sim, ".png", sep = ''), width = 4, height = 4)
```


## Welfare impacts per boat ramp
```{r welfare_bar, include = FALSE}
# RUM_W$BR <- as.factor(RUM_W$BR)
# 
# W_BRimpact <- RUM_W %>% group_by(BR, Ramp_name) %>%
#       summarise(round(across(starts_with(c("welf_low", "welf_mean","welf_upp")), sum))) %>% 
#   filter(welf_mean_TRIPS_ramp_prop_Grvt__5 > 0)
# 
# W_BRimpact_bar <- W_BRimpact %>%
#   ggplot(.) +
#   geom_bar(aes(x = BR, y = welf_mean_TRIPS_ramp_prop_Grvt__5), stat = "identity",  fill = col) +
#   geom_errorbar(aes(x = BR, ymin = welf_low_low_TRIPS_Grvt__5, ymax = welf_upp_upp_TRIPS_Grvt__5), width = 0.2) +
#   my_theme +
#   scale_x_discrete(drop = T, labels = W_BRimpact$Ramp_name) +
#   theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
#   ylab("Annual welfare impacts on\nrecreational fishers ($)") +
#   xlab("Boat Ramp")
# 
# W_BRimpact_bar
# ggsave(path = "./plots", filename = paste("sim1.W_BRimpact_bar.png", sep = ''), plot = W_BRimpact_bar, width = 8, height = 4)
# 
# 
# W_BRimpact %<>%  rename("Annual lower welfare impact ($)" = welf_low_low_TRIPS_Grvt__5,
#                         "Annual mean welfare impact ($)" = welf_mean_TRIPS_ramp_prop_Grvt__5,
#                         "Annual upper welfare impact ($)" = welf_upp_upp_TRIPS_Grvt__5)
# 
# W_BRimpact
# png("./plots/sim1.W_BRimpact.png", height = 50*nrow(W_BRimpact), width = 150*ncol(W_BRimpact))
# grid.table(W_BRimpact)
# dev.off()

```

```{r}
# for (i in W_BRimpact$BR) {
#   br <- BRtrips %>% filter(RampID %in% i)
# bbox <- RUM_grid %>%  filter(BR %in% i)
# mp_crop <- mp %>% st_crop(bbox)
# mp_comm <- NPZ %>% st_crop(bbox)
# 
# presim <- RUM_grid %>% filter(BR == i) %>%
#     ggplot(.) +
#       xlab("Longitude") + ylab("Latitude") +
#       geom_sf(aes(fill = mean_base_TRIPS_ramp_prop_Grvt__5), lwd = 0.05) +
#       scale_fill_distiller(palette = "YlGnBu", direction = 1) +
#       labs(fill = "Trips\nannually") +
#       my_theme +
#     geom_sf(data = mp_crop, alpha = 0, lwd = 0.25) +
#   geom_sf(data = mp_comm, alpha = 0, lwd = 0.25) +
#   ggtitle(paste(br$Ramp_name, " pre-simulation ")) +
#   geom_sf(data = br, color = 'red', fill = 'red', size = 1)
# 
# ggsave(path = "./plots", filename = paste("sim1.", br$Ramp_name,"_presim.png", sep = ''), plot = presim, width = 8, height = 4)
# print(presim)
# 
# sim <- RUM_grid %>% filter(BR == i & gridID_alt %in% ntz)
# 
# postsim <- RUM_grid %>% filter(BR == i) %>%
# ggplot(.) +
#       xlab("Longitude") + ylab("Latitude") +
#       geom_sf(aes(fill = mean_sim_TRIPS_ramp_prop_Grvt__5), lwd = 0.05) +
#       scale_fill_distiller(palette = "YlGnBu", direction = 1) +
#       labs(fill = "Trips\nannually") +
#   geom_sf(data = mp_crop, alpha = 0, lwd = 0.25) +
#   geom_sf(data = mp_comm, alpha = 0, lwd = 0.25) +
#   geom_sf(data = br, color = 'red', fill = 'red', size = 1) +
#   geom_sf(data = sim, fill = "black", lwd = 0.05) +
#   ggtitle(paste(br$Ramp_name, " post-simulation")) +
#   my_theme
# 
# ggsave(path = "./plots", filename = paste("sim1.", br$Ramp_name,"_postsim.png", sep = ''), plot = postsim, width = 8, height = 4)
# print(postsim)
# }
```

```{r difference plots}
# RUM_grid %<>%
#      mutate(mean_diff = mean_sim_TRIPS_ramp_prop_Grvt__5 - mean_base_TRIPS_ramp_prop_Grvt__5,
#             upp_diff = upp_sim_upp_TRIPS_Grvt__5 - upp_base_upp_TRIPS_Grvt__5,
#             low_diff = low_sim_low_TRIPS_Grvt__5 - low_base_low_TRIPS_Grvt__5)
# 
# for (i in W_BRimpact$BR) {
#   br <- BRtrips %>% filter(RampID %in% i)
#   bbox <- RUM_grid %>%  filter(BR %in% i)
#   mp_crop <- mp %>% st_crop(bbox)
#   mp_comm <- NPZ %>% st_crop(bbox)
#   sim <- RUM_grid %>% filter(BR == i & gridID_alt %in% ntz)
#   
# diff <- RUM_grid %>% filter(BR == i) %>%
#     ggplot(.) +
#       xlab("Longitude") + ylab("Latitude") +
#       geom_sf(aes(fill = mean_diff), lwd = 0.05) +
#       scale_fill_distiller(palette = "YlGnBu", direction = -1) +
#       labs(fill = "Change in\nannual trips") +
#       my_theme +
#     geom_sf(data = mp_crop, alpha = 0, lwd = 0.25) +
#     geom_sf(data = mp_comm, alpha = 0, lwd = 0.25) +
#       ggtitle(paste(br$Ramp_name)) +
#   geom_sf(data = sim, fill = "black", lwd = 0.05) +
#     geom_sf(data = br, color = 'red', fill = 'red', size = 1)
# 
# ggsave(path = "./plots", filename = paste("sim1.", br$Ramp_name,"_diff.png", sep = ''), plot = diff, width = 8, height = 4)
# print(diff)
# }
```